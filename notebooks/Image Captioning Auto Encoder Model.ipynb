{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5qiQ5veicum"
      },
      "source": [
        "### Import Necessary Libraries\n",
        "\n",
        "In this cell, we import the necessary libraries for our image captioning model. These libraries include PyTorch for building and training the model, and torchvision for pre-trained models and transformations.\n",
        "\n",
        "1. **`torch`**: This is the main PyTorch library which provides the core functionalities for tensor operations, neural network layers, and optimization.\n",
        "2. **`torch.nn`**: This module contains classes and functions to build neural networks. It includes pre-defined layers like `Linear` and `Dropout`, as well as loss functions like `CrossEntropyLoss`.\n",
        "3. **`torchvision.models`**: This module provides access to pre-trained models and model architectures. Here, we use it to import `ResNet18`, a commonly used model for image classification tasks.\n",
        "4. **`os`**: Provides functions to interact with the operating system, like file path manipulation.\n",
        "5. **`pandas`**: A library for data manipulation and analysis, especially useful for handling tabular data.\n",
        "6. **`keras_preprocessing.text`**: A library for handling various transformations over text.\n",
        "7. **`Counter`**: A class from collections used for evaluation of frequencies of words in a text.\n",
        "8. **`DataLoader` and `Dataset`**: PyTorch utilities for loading and managing data efficiently.\n",
        "9. **`Image`**: A class from PIL (Python Imaging Library) used to open and manipulate images.\n",
        "10. **`matplotlib`**: A library for plotting and visualization of models and images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-mWw-oninhl",
        "outputId": "4d3bf0af-7554-4bb5-c0d5-8933dad0d967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RKqXUT2oicun"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.models.resnet as resnet\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.utils import Progbar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8VeyiXnicuo"
      },
      "source": [
        "### Define Constants\n",
        "\n",
        "This cell defines several constants used throughout the notebook. Constants help maintain consistency and simplify code updates.\n",
        "\n",
        "1. **`START_TOKEN`, `END_TOKEN`, `PAD_TOKEN`, `UNKNOWN_TOKEN`**: These are special tokens used in text processing. They represent the start of a sequence, end of a sequence, padding, and unknown words, respectively.\n",
        "2. **`EMBEDDING_SIZE`**: This defines the size of the word embeddings used in the model. Larger sizes can capture more information but require more computational resources.\n",
        "3. **`HIDDEN_SIZE`**: This is the number of features in the hidden state of the LSTM (Long Short-Term Memory) network. It determines the capacity of the LSTM to capture dependencies in sequences.\n",
        "4. **`NUM_LAYERS`**: This represents the number of LSTM layers stacked on top of each other. More layers can help the model learn more complex patterns.\n",
        "5. **`BATCH_SIZE`**: This defines how many samples are processed together in one forward/backward pass. Larger batches can speed up training but require more memory.\n",
        "6. **`NUM_EPOCHS`**: This is the number of times the entire training dataset is passed through the model during training.\n",
        "7. **`LEARNING_RATE`**: This is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lQ0Qkwc3icup"
      },
      "outputs": [],
      "source": [
        "START_TOKEN = \"<START>\"\n",
        "END_TOKEN = \"<END>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "UNKOWN_TOKEN = \"<UNKOWN>\"\n",
        "EMBEDDING_SIZE = 300\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 1\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "FOLDER_PATH = r\"F:\\Programming\\Python\\Deep Learning\\Image-Captioning\\data\\raw\"\n",
        "IMAGES_FOLDER = r\"F:\\Programming\\Python\\Deep Learning\\Image-Captioning\\data\\raw\\Images\"\n",
        "CAPTIONS_FOLDER = r\"F:\\Programming\\Python\\Deep Learning\\Image-Captioning\\data\\raw\\captions.txt\"\n",
        "CHECKPOINT_DIR = r\"F:\\Programming\\Python\\Deep Learning\\Image-Captioning\\src\\training\\checkpoints\"\n",
        "RUNS_DIR = r\"F:\\Programming\\Python\\Deep Learning\\Image-Captioning\\src\\training\\runs\"\n",
        "DATA_DIR = r\"F:\\Programming\\Python\\Deep Learning\\Image-Captioning\\data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdj_jCUjicuq"
      },
      "source": [
        "### Image Captioning Dataset\n",
        "\n",
        "This class constructs a `Dataset` using the given images and captions that can be used by Dataloader to train the Model.\n",
        "\n",
        "1. **`INIT`**: The initial function to construct some initial parameters.\n",
        "    - `self.mapping`: A dictionary mapping each image to its corresponding list of captions.\n",
        "    - `self.all_captions`: A list to store all the preprocessed captions.\n",
        "    - `self.vocab`: A dictionary in which we keep every word in our vocabulary and give a unique index to each of them.\n",
        "    - `self.tokenizer`: A Module used to tokenize our captions.\n",
        "2. **`load_captions`**: Loads the captions from the provided file and creates the `mapping` dictionary.\n",
        "3. **`build_vocab`**: Function to build the vocabulary from the captions based on their frequencies.\n",
        "4. **`preprocess_captions`**: Preprocesses the captions by adding start and end tokens, and cleans the captions by removing punctuation and converting them to lowercase.\n",
        "5. **`calculate_max_length`**: Calculates the maximum length of all captions to be used for padding.\n",
        "6. **`pad_captions`**: Pads all captions to the maximum length using the maximum length.\n",
        "7. **`__len__`**: Returns the total length of the dataset.\n",
        "8. **`__getitem__`**: Returns a tuple containing the image and its corresponding caption after preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vkK0h2Ikicuq"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, img_dir, captions_file, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.mapping = self.load_captions(captions_file)\n",
        "        self.all_captions = []\n",
        "        self.preprocess_captions()\n",
        "        self.vocab = self.build_vocab(self.mapping)\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.tokenizer.fit_on_texts(self.all_captions)\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.max_length = self.calculate_max_length()\n",
        "        self.pad_captions()\n",
        "\n",
        "\n",
        "    def load_captions(self, captions_file):\n",
        "        captions = {}\n",
        "        with open(captions_file, 'r') as f:\n",
        "            next(f)\n",
        "            for line in f:\n",
        "                tokens = line.split(',')\n",
        "                img_id = tokens[0]\n",
        "                caption = ''.join(tokens[1:]).strip()\n",
        "                if img_id not in captions:\n",
        "                    captions[img_id] = []\n",
        "                captions[img_id].append(caption)\n",
        "        return captions\n",
        "\n",
        "\n",
        "    def build_vocab(self, captions):\n",
        "        tokens = [cap.split() for caps in captions.values() for cap in caps]\n",
        "        freq = Counter([item for sublist in tokens for item in sublist])\n",
        "        vocab = {word: idx for idx, (word, count) in enumerate(freq.items()) if count >= 1}\n",
        "        vocab[END_TOKEN] = len(vocab)\n",
        "        vocab[PAD_TOKEN] = len(vocab) + 1\n",
        "        with open(os.path.join(DATA_DIR, 'vocab.json'), \"w\") as vocab_file: \n",
        "            json.dump(vocab, vocab_file)\n",
        "        return vocab\n",
        "\n",
        "\n",
        "    def preprocess_captions(self):\n",
        "        for key, captions in self.mapping.items():\n",
        "            for i in range(len(captions)):\n",
        "                captions[i] = captions[i].lower().replace('.', '').replace(',', '').replace('!', '').replace('?', '').replace('\\\"', '')\n",
        "                captions[i] = START_TOKEN + \" \" + captions[i] + \" \" + END_TOKEN\n",
        "                self.all_captions.append(captions[i])\n",
        "\n",
        "\n",
        "    def calculate_max_length(self):\n",
        "        return max(len(caption.split()) for caption in self.all_captions)\n",
        "\n",
        "\n",
        "    def pad_captions(self):\n",
        "        for key, captions in self.mapping.items():\n",
        "            for i in range(len(captions)):\n",
        "                caption_len = len(captions[i].split())\n",
        "                for _ in range(self.max_length - caption_len):\n",
        "                    captions[i] += \" \" + PAD_TOKEN\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mapping)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_idx = idx//5\n",
        "        img_id = list(self.mapping.keys())[image_idx]\n",
        "        img_path = os.path.join(self.img_dir, img_id)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption_idx = idx%5\n",
        "        caption = self.mapping[img_id][caption_idx]\n",
        "\n",
        "        tokens = caption.split()\n",
        "        tokenized_caption = [self.vocab.get(token) for token in tokens]\n",
        "\n",
        "        return image, torch.tensor(tokenized_caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_J8xDMticuq"
      },
      "source": [
        "### Encoder Class\n",
        "\n",
        "This cell defines the `Encoder` class, which is part of the image captioning model. The encoder takes images as input and generates feature representations.\n",
        "\n",
        "1. **`__init__` Method**: This constructor initializes the encoder. It uses a pre-trained ResNet18 model from `torchvision.models` and modifies its final fully connected layer to match the desired embedding size.\n",
        "   - **`self.ResNet18`**: Uses a pre-trained ResNet18 model from `torchvision.models`.\n",
        "   - **`self.ResNet18.fc`**: Replaces the final fully connected layer of ResNet18 to output features of size `embedding_size`.\n",
        "   - **`self.ReLU`**: Applies the ReLU activation function to the output features to introduce non-linearity.\n",
        "   - **`self.Dropout`**: Applies dropout to prevent overfitting by randomly setting a fraction of the features to zero during training.\n",
        "\n",
        "2. **`forward` Method**: This method defines the forward pass of the encoder. It takes images as input and returns the processed features.\n",
        "   - **`features = self.ResNet18(images)`**: Passes the images through ResNet18 to get the feature representations.\n",
        "   - **`return self.Dropout(self.ReLU(features))`**: Applies ReLU activation and dropout to the output features before returning them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "39uvve8hicuq"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.ResNet18 = resnet.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT, progress=True)\n",
        "        self.ResNet18.fc = nn.Linear(self.ResNet18.fc.in_features, embedding_size)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.Dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.ResNet18(images)\n",
        "        return self.Dropout(self.ReLU(features))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGRNRWTjicur"
      },
      "source": [
        "### Decoder Class\n",
        "\n",
        "This cell defines the `Decoder` class, which is responsible for generating captions from the features produced by the encoder. It uses an LSTM network to process sequential data.\n",
        "\n",
        "1. **`__init__` Method**: This constructor initializes the decoder.\n",
        "   - **`self.embed`**: An embedding layer that converts word indices into dense vectors of size `embedding_size`.\n",
        "   - **`self.LSTM`**: An LSTM layer that processes the embedded captions along with image features. It has `hidden_size` units and `num_layers` layers.\n",
        "   - **`self.Linear`**: A linear layer that maps the LSTM output to vocabulary size to predict the next word.\n",
        "   - **`self.Dropout`**: Applies dropout to the embeddings to prevent overfitting.\n",
        "\n",
        "2. **`forward` Method**: This method defines the forward pass of the decoder.\n",
        "   - **`embeddings = self.Dropout(self.embed(captions))`**: Converts captions into embeddings and applies dropout.\n",
        "   - **`features = features.unsqueeze(1)`**: Adds a batch dimension to the image features. The batch size is assumed to be 1 for simplicity.\n",
        "   - **`embeddings = torch.cat((features, embeddings), dim=1)`**: Concatenates image features with the caption embeddings. The image features are added as the first step of the sequence.\n",
        "   - **`hiddens, _ = self.LSTM(embeddings)`**: Passes the sequence through the LSTM.\n",
        "   - **`outputs = self.Linear(hiddens)`**: Projects the LSTM output to vocabulary size.\n",
        "   - **`return outputs`**: Returns the predicted outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FtwqckI4icur"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, vocab_size, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.LSTM = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bias=True)\n",
        "        self.Linear = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
        "        self.Dropout = nn.Dropout(0.5)\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = self.Dropout(embeddings)\n",
        "        features = features.unsqueeze(1)\n",
        "        embeddings = torch.cat((features, embeddings), dim=1)\n",
        "        embeddings = embeddings.reshape(embeddings.shape[1], embeddings.shape[0], embeddings.shape[-1])\n",
        "        hiddens, _ = self.LSTM(embeddings)\n",
        "        return self.Linear(hiddens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxTfsqrpicur"
      },
      "source": [
        "### ImageCaptioningModel Class\n",
        "\n",
        "This cell defines the `ImageCaptioningModel` class, which combines the encoder and decoder into a single model for generating image captions.\n",
        "\n",
        "1. **`__init__` Method**: This constructor initializes the image captioning model.\n",
        "   - **`self.Encoder`**: An instance of the `Encoder` class which extracts features from images.\n",
        "   - **`self.Decoder`**: An instance of the `Decoder` class which generates captions from image features and previous captions.\n",
        "\n",
        "2. **`forward` Method**: This method defines the forward pass of the combined model.\n",
        "   - **`features = self.Encoder(images)`**: Extracts features from the input images using the encoder.\n",
        "   - **`outputs = self.Decoder(features, captions)`**: Generates captions based on the extracted features and input captions using the decoder.\n",
        "   - **`return outputs`**: Returns the predicted outputs.\n",
        "\n",
        "3. **`caption` Method**: This method generates a caption for a given image.(Although this method is not used in the provided code.)\n",
        "   - **`features = self.Encoder(image).unsqueeze(0)`**: Extracts features from the input image and adds a batch dimension.\n",
        "   - **`states`**: Keeps track of the hidden states of the LSTM during caption generation.\n",
        "   - **`for _ in range(max_length)`**: Iteratively generates words for the caption up to `max_length` words.\n",
        "   - **`hiddens, states = self.Decoder.LSTM(features, states)`**: Passes the image features and previous states through the LSTM.\n",
        "   - **`output = self.Decoder.Linear(hiddens.squeeze(0))`**: Projects the LSTM output to vocabulary size.\n",
        "   - **`predicted = output.argmax(1)`**: Gets the index of the highest probability word.\n",
        "   - **`final_caption.append(predicted.item())`**: Adds the predicted word index to the caption list.\n",
        "   - **`features = self.Decoder.embed(predicted).unsqueeze(0)`**: Updates the image features with the newly predicted word embedding.\n",
        "   - **`if vocab.itos[predicted.item()] == END_TOKEN`**: Checks if the end token was predicted, indicating the end of the caption.\n",
        "   - **`return [vocab.itos[idx] for idx in final_caption]`**: Converts the list of word indices back to words and returns the final caption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PwJQP-BHicur"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, vocab_size, num_layers):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "        self.Encoder = Encoder(embedding_size)\n",
        "        self.Decoder = Decoder(embedding_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.Encoder(images)\n",
        "        outputs = self.Decoder(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption(self, image, vocab, max_length=40):\n",
        "        final_caption = []\n",
        "        with torch.no_grad():\n",
        "            features = self.Encoder(image).unsqueeze(0)\n",
        "            states = None\n",
        "            for _ in range(max_length):\n",
        "                hiddens, states = self.Decoder.LSTM(features, states)\n",
        "                output = self.Decoder.Linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(1)\n",
        "\n",
        "                final_caption.append(predicted.item())\n",
        "                features = self.Decoder.embed(predicted).unsqueeze(0)\n",
        "                if vocab.itos[predicted.item()] == END_TOKEN:\n",
        "                    break\n",
        "\n",
        "        return [vocab.itos[idx] for idx in final_caption]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8TAnK3Micur"
      },
      "source": [
        "### Model Training\n",
        "\n",
        "This is where we define the training loop for the image captioning model but first, we need to prepare the dataset, transform and dataloader.\\n\n",
        "Then, we define the model, according to the given hyperparameters.\n",
        "\n",
        "1. **Dataset Preparation**: Load the dataset, preprocess the images and captions, and create a vocabulary.\n",
        "2. **DataLoader Creation**: Create a dataloader to load the dataset in batches.\n",
        "3. **Model Creation**: Create an instance of the `ImageCaptioningModel` class, specifying the hyperparameters.\n",
        "4. **Training Loop**: Iterate over the dataset, pass the images through the model, calculate the loss, backpropagate the gradients, and update the model parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybfev8HWicur",
        "outputId": "c2ee303a-633d-43d0-e38e-f3ba4ca6b94e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "Epoch 1/10\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 2s/step - Loss: 1.4226\n",
            "Epoch 2/10\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 2s/step - Loss: 1.4680\n",
            "Epoch 3/10\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 2s/step - Loss: 1.2441\n",
            "Epoch 4/10\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 2s/step - Loss: 1.0723\n",
            "Epoch 5/10\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 1s/step - Loss: 1.0286\n",
            "Epoch 6/10\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 1s/step - Loss: 1.0060\n",
            "Epoch 7/10\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 1s/step - Loss: 1.0368\n",
            "Epoch 8/10\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 1s/step - Loss: 0.9494\n",
            "Epoch 9/10\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 1s/step - Loss: 0.9970\n",
            "Epoch 10/10\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 1s/step - Loss: 1.1012\n"
          ]
        }
      ],
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "dataset = ImageCaptionDataset(img_dir=IMAGES_FOLDER, captions_file=CAPTIONS_FOLDER, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "model = ImageCaptioningModel(embedding_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=dataset.vocab_size, num_layers=NUM_LAYERS)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "    model.train()\n",
        "    data_size = len(dataloader)\n",
        "    pb_i = Progbar(data_size, stateful_metrics=['Loss'])\n",
        "\n",
        "    for images, captions in (dataloader):\n",
        "        outputs = model(images, captions[:,:-1])\n",
        "        loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        values=[('Loss', loss.item())]\n",
        "        pb_i.add(1, values=values)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqurtOKkicus"
      },
      "source": [
        "### Saving the Model\n",
        "\n",
        "This cell saves the trained model to a file named 'image_captioning_model.pth'. This model can then be used to generate captions for new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "maZTYeEDicus"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), os.path.join(RUNS_DIR, 'image_captioning_model_1.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv0HriiHicus"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "This cell evaluates the model's performance on a separate test dataset.\n",
        "\n",
        "- **`generate_caption`**: This function will take a test dataset and predict the caption for a given image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txUSdO3ticus"
      },
      "outputs": [],
      "source": [
        "def generate_caption(image_path, model, dataset):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0)\n",
        "    caption = model.caption(image, dataset.vocab)\n",
        "    return ' '.join(caption)\n",
        "\n",
        "example_image_path = 'Dataset/Images/example.jpg'\n",
        "caption = generate_caption(example_image_path, model, dataset)\n",
        "print(caption)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "nbformat": 4,
    "nbformat_minor": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
